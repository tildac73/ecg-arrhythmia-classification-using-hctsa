{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Arrhythmia Classification Pipeline\n",
    "\n",
    "**Complete pipeline for ECG analysis:**\n",
    "1. Feature extraction from ECG signals\n",
    "2. Preprocessing and filtering\n",
    "3. Class balancing (SMOTE)\n",
    "4. Random Forest classification\n",
    "5. Performance evaluation\n",
    "6. SHAP interpretability analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#!/usr/bin/env python3\nimport os\nimport sys\nimport time\nimport warnings\nimport gc\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport scipy.io as sio\nimport scipy.signal as signal\nimport pywt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report, accuracy_score,\n    roc_curve, auc, precision_recall_curve, average_precision_score\n)\nfrom sklearn.preprocessing import LabelBinarizer\n\n# Imbalanced learning\nfrom imblearn.over_sampling import SMOTE\n\n# Feature extraction\ntry:\n    import pycatch22\n    USING_PYCATCH22 = True\nexcept ImportError:\n    USING_PYCATCH22 = False\n    print(\"Warning: pycatch22 not available. Install with: pip install pycatch22\")\n\n# SHAP for interpretability\ntry:\n    import shap\n    USING_SHAP = True\nexcept ImportError:\n    USING_SHAP = False\n    print(\"Warning: SHAP not available. Install with: pip install shap\")\n\nwarnings.filterwarnings('ignore')\n\n# Enable inline plotting\n%matplotlib inline\n\nprint(\"All imports successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters\n",
    "\n",
    "Adjust these parameters to customize the analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CONFIGURATION\n# ============================================================================\nDATA_DIR = \"./ecg_arrhythmia_data/processed_ecg_signals_2/WFDBRecords\"\nOUTPUT_DIR = \"./ecg_analysis_results\"\n\n# Preprocessing options\nAPPLY_PREPROCESSING = True\nAPPLY_BALANCING = True\n\n# Signal processing parameters\nSAMPLING_RATE = 500\nBASELINE_CUTOFF = 0.5\nLOWPASS_CUTOFF = 40\nNOTCH_FREQ = 60\nWAVELET_TYPE = 'db6'\nWAVELET_LEVEL = 3\n\n# Machine learning parameters\nN_ESTIMATORS = 100\nTEST_SIZE = 0.3\nRANDOM_STATE = 42\n\n# Visualization parameters\nSHAP_SAMPLE_SIZE = 300\nTOP_N_FEATURES = 20\nFIGURE_DPI = 150\nFONT_SIZE = 12\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(\"=\"*70)\nprint(\"ECG ANALYSIS PIPELINE\")\nprint(\"=\"*70)\nprint(f\"\\nData directory: {DATA_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Preprocessing: {'Enabled' if APPLY_PREPROCESSING else 'Disabled'}\")\nprint(f\"SMOTE balancing: {'Enabled' if APPLY_BALANCING else 'Disabled'}\")\nprint(f\"Random Forest estimators: {N_ESTIMATORS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Functions\n",
    "\n",
    "Functions for ECG signal preprocessing and SNOMED-CT to AAMI class mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def preprocess_ecg_signal(ecg_signal, fs=SAMPLING_RATE):\n    \"\"\"Preprocess ECG signal with filtering and wavelet denoising.\"\"\"\n    if not APPLY_PREPROCESSING:\n        return (ecg_signal - np.mean(ecg_signal)) / (np.std(ecg_signal) + 1e-10)\n\n    try:\n        # Baseline wander removal\n        b, a = signal.butter(1, BASELINE_CUTOFF / (0.5 * fs), btype='highpass')\n        processed = signal.filtfilt(b, a, ecg_signal)\n\n        # Low-pass filter\n        b, a = signal.butter(4, LOWPASS_CUTOFF / (0.5 * fs), btype='low')\n        processed = signal.filtfilt(b, a, processed)\n\n        # Notch filter\n        b, a = signal.iirnotch(NOTCH_FREQ / (0.5 * fs), 30)\n        processed = signal.filtfilt(b, a, processed)\n\n        # Wavelet denoising\n        coeffs = pywt.wavedec(processed, WAVELET_TYPE, level=WAVELET_LEVEL)\n        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n        uthresh = sigma * np.sqrt(2 * np.log(len(processed)))\n        denoised_coeffs = list(map(lambda x: pywt.threshold(x, uthresh, mode='soft'), coeffs))\n        processed = pywt.waverec(denoised_coeffs, WAVELET_TYPE)\n\n        # Normalise\n        return (processed - np.mean(processed)) / (np.std(processed) + 1e-10)\n    except:\n        return (ecg_signal - np.mean(ecg_signal)) / (np.std(ecg_signal) + 1e-10)\n\nprint(\"Preprocessing function defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def map_snomed_to_aami(snomed_codes, db_path=\"./ecg_arrhythmia_data/ecg-arrhythmia-1.0.0\"):\n    \"\"\"Map SNOMED-CT diagnostic codes to AAMI arrhythmia classes.\"\"\"\n    snomed_path = os.path.join(db_path, \"ConditionNames_SNOMED-CT.csv\")\n    snomed_dict = {}\n\n    if os.path.exists(snomed_path):\n        try:\n            snomed_df = pd.read_csv(snomed_path)\n            for _, row in snomed_df.iterrows():\n                snomed_dict[str(row['Snomed_CT'])] = row['Acronym Name']\n        except:\n            pass\n\n    aami_map = {\n        'SR': 'N', 'SB': 'N', 'ST': 'N', 'SA': 'N', 'NSR': 'N',\n        'AFIB': 'S', 'AF': 'S', 'AT': 'S', 'AVNRT': 'S', 'AVRT': 'S',\n        'SVT': 'S', 'APB': 'S', 'ABI': 'S', 'SAAWR': 'S', 'JPT': 'S',\n        'JEB': 'S', '1AVB': 'S', '2AVB': 'S', '2AVB1': 'S', '2AVB2': 'S', 'AVB': 'S',\n        'VPB': 'V', 'VEB': 'V', 'VB': 'V', 'VET': 'V', 'VPE': 'V',\n        '3AVB': 'V', 'RBBB': 'V', 'LBBB': 'V', 'LFBBB': 'V', 'LBBBB': 'V',\n        'IVB': 'V', 'IDC': 'V',\n        'VFW': 'F',\n    }\n\n    aami_classes = []\n    for code in snomed_codes:\n        code = str(code).strip()\n        if code in snomed_dict:\n            acronym = snomed_dict[code]\n            aami_class = aami_map.get(acronym, 'Q')\n            aami_classes.append(aami_class)\n        else:\n            aami_classes.append('Q')\n\n    if 'V' in aami_classes:\n        return 'V'\n    if 'S' in aami_classes:\n        return 'S'\n    if 'F' in aami_classes:\n        return 'F'\n    if 'Q' in aami_classes:\n        return 'Q'\n    return 'N'\n\nprint(\"SNOMED-CT mapping function defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_features_from_ecg(file_path):\n    \"\"\"Extract catch22 time-series features from ECG signals.\"\"\"\n    features = []\n    try:\n        hea_path = file_path.replace('.mat', '.hea')\n        aami_class = 'Q'\n        dx_codes = []\n\n        if os.path.exists(hea_path):\n            with open(hea_path, 'r') as f:\n                for line in f:\n                    if line.startswith('#Dx:'):\n                        dx_str = line.strip().replace('#Dx:', '')\n                        dx_codes = [code.strip() for code in dx_str.split(',')]\n                        break\n\n        if dx_codes:\n            aami_class = map_snomed_to_aami(dx_codes)\n\n        mat_data = sio.loadmat(file_path)\n        signal_data = None\n\n        for key in ['val', 'data', 'signal']:\n            if key in mat_data:\n                signal_data = mat_data[key]\n                break\n\n        if signal_data is None:\n            for key, value in mat_data.items():\n                if isinstance(value, np.ndarray) and value.size > 0 and not key.startswith('__'):\n                    signal_data = value\n                    break\n\n        if signal_data is None:\n            return None\n\n        if signal_data.shape[0] > signal_data.shape[1]:\n            signal_data = signal_data.T\n\n        file_id = os.path.basename(file_path).replace('.mat', '')\n        n_leads = signal_data.shape[0]\n\n        for lead_idx in range(n_leads):\n            lead_signal = signal_data[lead_idx, :].astype(float)\n\n            if len(lead_signal) < 10 or np.std(lead_signal) < 1e-10:\n                continue\n\n            if np.count_nonzero(lead_signal) < 0.5 * len(lead_signal):\n                continue\n\n            try:\n                lead_signal_processed = preprocess_ecg_signal(lead_signal)\n\n                if USING_PYCATCH22:\n                    catch22_result = pycatch22.catch22_all(lead_signal_processed)\n                    feature_values = catch22_result['values']\n                    feature_names = catch22_result['names']\n                else:\n                    feature_dict = {f\"feature_{i}\": np.random.randn() for i in range(22)}\n                    feature_values = list(feature_dict.values())\n                    feature_names = list(feature_dict.keys())\n\n                for feat_name, feat_val in zip(feature_names, feature_values):\n                    if feat_val is None or (isinstance(feat_val, (int, float)) and (np.isinf(feat_val) or np.isnan(feat_val))):\n                        feat_val = np.nan\n\n                    features.append({\n                        'record_id': file_id,\n                        'lead': lead_idx,\n                        'feature_name': str(feat_name),\n                        'feature_value': float(feat_val) if not np.isnan(feat_val) else np.nan,\n                        'class': aami_class\n                    })\n            except:\n                pass\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n    return features\n\nprint(\"Feature extraction function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction from ECG Files\n",
    "\n",
    "Process all ECG files and extract catch22 time-series features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 1: FEATURE EXTRACTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find all .mat files with corresponding .hea files\n",
    "mat_files = []\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith('.mat') and not file.startswith('.'):\n",
    "            hea_file = file.replace('.mat', '.hea')\n",
    "            if os.path.exists(os.path.join(root, hea_file)):\n",
    "                mat_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(mat_files)} ECG files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract features from all files\nall_features = []\nclass_counts = Counter()\n\nfor i, file_path in enumerate(mat_files):\n    if (i + 1) % 10 == 0 or (i + 1) == len(mat_files):\n        print(f\"Processing {i+1}/{len(mat_files)}: {os.path.basename(file_path)}\")\n    \n    features = extract_features_from_ecg(file_path)\n    if features:\n        all_features.extend(features)\n        class_counts[features[0]['class']] += 1\n\nfeatures_df = pd.DataFrame(all_features)\nprint(f\"\\nExtracted {len(features_df)} feature values\")\nprint(f\"Class distribution: {dict(class_counts)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Preparation\n",
    "\n",
    "Transform features into wide format and prepare for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"STEP 2: FEATURE PREPARATION\")\nprint(\"=\"*70)\n\n# Pivot to wide format\nfeatures_wide = features_df.pivot_table(\n    index='record_id',\n    columns='feature_name',\n    values='feature_value',\n    aggfunc='first'\n).reset_index()\n\nclass_mapping = features_df.groupby('record_id')['class'].first()\nfeatures_wide['class'] = features_wide['record_id'].map(class_mapping)\n\nprint(f\"Feature matrix shape: {features_wide.shape}\")\n\n# Display first few rows\ndisplay(features_wide.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Separate features and target\nfeature_cols = [col for col in features_wide.columns if col not in ['record_id', 'class']]\nX = features_wide[feature_cols].values\ny = features_wide['class'].values\n\n# Impute missing values\nimputer = SimpleImputer(strategy='mean')\nX = imputer.fit_transform(X)\n\nprint(f\"Features: {X.shape}\")\nprint(f\"Original class counts: {Counter(y)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Class Balancing with SMOTE\n",
    "\n",
    "Apply Synthetic Minority Over-sampling Technique to balance classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"STEP 3: CLASS BALANCING\")\nprint(\"=\"*70)\n\nif APPLY_BALANCING:\n    try:\n        class_counts = Counter(y)\n        min_samples = min(class_counts.values())\n\n        if min_samples >= 2:\n            k_neighbors = min(1, min_samples - 1)\n\n            smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=k_neighbors)\n            X_balanced, y_balanced = smote.fit_resample(X, y)\n\n            print(f\"Applied SMOTE balancing\")\n            print(f\"Balanced class counts: {Counter(y_balanced)}\")\n        else:\n            print(\"Not enough samples for SMOTE, using original data\")\n            X_balanced, y_balanced = X, y\n    except Exception as e:\n        print(f\"SMOTE failed ({e}), using original data\")\n        X_balanced, y_balanced = X, y\nelse:\n    print(\"Skipping balancing\")\n    X_balanced, y_balanced = X, y"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"STEP 4: TRAIN RANDOM FOREST CLASSIFIER\")\nprint(\"=\"*70)\n\n# Split data\nclass_counts = Counter(y_balanced)\nmin_class_count = min(class_counts.values())\n\nif min_class_count >= 2:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_balanced, y_balanced,\n        test_size=TEST_SIZE,\n        random_state=RANDOM_STATE,\n        stratify=y_balanced\n    )\nelse:\n    print(\"Too few samples for stratified split, using random split\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_balanced, y_balanced,\n        test_size=TEST_SIZE,\n        random_state=RANDOM_STATE\n    )\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train Random Forest\nprint(f\"\\nTraining Random Forest (n_estimators={N_ESTIMATORS})...\")\nrf_classifier = RandomForestClassifier(\n    n_estimators=N_ESTIMATORS,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\nrf_classifier.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = rf_classifier.predict(X_test_scaled)\ny_pred_proba = rf_classifier.predict_proba(X_test_scaled)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nTest Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics\n",
    "\n",
    "Calculate detailed performance metrics including sensitivity, specificity, PPV, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"STEP 5: EVALUATION METRICS\")\nprint(\"=\"*70)\n\nclass_labels = sorted(np.unique(y_balanced))\ncm = confusion_matrix(y_test, y_pred, labels=class_labels)\n\nprint(\"\\nConfusion Matrix:\")\nprint(cm)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate detailed metrics\nmetrics_list = []\nfor i, label in enumerate(class_labels):\n    tp = cm[i, i]\n    fn = np.sum(cm[i, :]) - tp\n    fp = np.sum(cm[:, i]) - tp\n    tn = np.sum(cm) - (tp + fn + fp)\n\n    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n    f1 = 2 * (ppv * sensitivity) / (ppv + sensitivity) if (ppv + sensitivity) > 0 else 0\n\n    metrics_list.append({\n        'Class': label,\n        'Sensitivity': sensitivity,\n        'Specificity': specificity,\n        'PPV': ppv,\n        'F1_Score': f1\n    })\n\nmetrics_df = pd.DataFrame(metrics_list)\nprint(\"\\nDetailed Metrics:\")\ndisplay(metrics_df.round(4))\n\n# Save results\nmetrics_df.to_csv(os.path.join(OUTPUT_DIR, 'performance_metrics.csv'), index=False)\nprint(f\"\\nSaved metrics to: {OUTPUT_DIR}/performance_metrics.csv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizations\n",
    "\n",
    "Generate confusion matrices, ROC curves, and feature importance plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 6: GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "plt.rcParams.update({'font.size': FONT_SIZE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_labels, yticklabels=class_labels,\n            square=True, ax=axes[0])\naxes[0].set_xlabel('Predicted', fontweight='bold')\naxes[0].set_ylabel('True', fontweight='bold')\naxes[0].set_title('Confusion Matrix (Counts)', fontweight='bold')\n\ncm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Greens',\n            xticklabels=class_labels, yticklabels=class_labels,\n            square=True, ax=axes[1])\naxes[1].set_xlabel('Predicted', fontweight='bold')\naxes[1].set_ylabel('True', fontweight='bold')\naxes[1].set_title('Confusion Matrix (Normalized)', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrices.png'), dpi=FIGURE_DPI)\nprint(\"Saved: confusion_matrices.png\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if len(class_labels) > 1:\n    lb = LabelBinarizer()\n    y_test_bin = lb.fit_transform(y_test)\n\n    plt.figure(figsize=(10, 8))\n    colors = ['blue', 'red', 'green', 'purple', 'orange']\n\n    for i, (color, label) in enumerate(zip(colors[:len(class_labels)], class_labels)):\n        if i < y_test_bin.shape[1] and i < y_pred_proba.shape[1]:\n            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n            roc_auc = auc(fpr, tpr)\n            plt.plot(fpr, tpr, color=color, lw=2,\n                     label=f'Class {label} (AUC = {roc_auc:.3f})')\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontweight='bold')\n    plt.ylabel('True Positive Rate', fontweight='bold')\n    plt.title('ROC Curves', fontweight='bold')\n    plt.legend(loc=\"lower right\")\n    plt.grid(alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'roc_curves.png'), dpi=FIGURE_DPI)\n    print(\"Saved: roc_curves.png\")\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "feature_importance = rf_classifier.feature_importances_\nimportance_df = pd.DataFrame({\n    'Feature': feature_cols,\n    'Importance': feature_importance\n}).sort_values('Importance', ascending=False)\n\nplt.figure(figsize=(10, 8))\ntop_n = min(20, len(importance_df))\ntop_features = importance_df.head(top_n)\nplt.barh(range(len(top_features)), top_features['Importance'].values[::-1])\nplt.yticks(range(len(top_features)), top_features['Feature'].values[::-1])\nplt.xlabel('Importance', fontweight='bold')\nplt.ylabel('Feature', fontweight='bold')\nplt.title(f'Top {top_n} Feature Importance', fontweight='bold')\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'feature_importance.png'), dpi=FIGURE_DPI)\nprint(\"Saved: feature_importance.png\")\nplt.show()\n\nimportance_df.to_csv(os.path.join(OUTPUT_DIR, 'feature_importance.csv'), index=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\ndisplay(importance_df.head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SHAP Analysis (Interpretability)\n",
    "\n",
    "Use SHAP (SHapley Additive exPlanations) to understand feature contributions to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USING_SHAP:\n    print(\"=\"*70)\n    print(\"STEP 7: SHAP ANALYSIS\")\n    print(\"=\"*70)\n\n    sample_size = min(SHAP_SAMPLE_SIZE, len(X_test_scaled))\n    X_shap_sample = X_test_scaled[:sample_size]\n    y_shap_sample = y_test[:sample_size]\n\n    print(f\"\\nCalculating SHAP values for {sample_size} samples...\")\n    print(\"This may take several minutes...\")\n\n    explainer = shap.TreeExplainer(rf_classifier)\n    shap_values = explainer.shap_values(X_shap_sample)\n\n    X_shap_df = pd.DataFrame(X_shap_sample, columns=feature_cols)\n    \n    print(\"SHAP values calculated!\")\nelse:\n    print(\"=\"*70)\n    print(\"STEP 7: SHAP ANALYSIS (SKIPPED)\")\n    print(\"=\"*70)\n    print(\"SHAP not available. Install with: pip install shap\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 SHAP Summary Plot (All Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USING_SHAP:\n    print(\"\\nGenerating SHAP summary plot...\")\n    plt.figure(figsize=(14, 12))\n    shap.summary_plot(shap_values, X_shap_df, class_names=class_labels,\n                     show=False, max_display=TOP_N_FEATURES)\n    plt.title('SHAP Feature Importance - All Classes', fontsize=16, fontweight='bold', pad=20)\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'shap_summary_all_classes.png'), dpi=FIGURE_DPI)\n    print(\"Saved: shap_summary_all_classes.png\")\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 SHAP Class-Specific Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USING_SHAP:\n    print(\"\\nGenerating class-specific SHAP plots...\")\n    for i, class_name in enumerate(class_labels):\n        plt.figure(figsize=(12, 10))\n        shap.summary_plot(shap_values[i], X_shap_df, show=False, max_display=TOP_N_FEATURES)\n        plt.title(f'SHAP Feature Importance - Class {class_name}', fontsize=16, fontweight='bold', pad=20)\n        plt.tight_layout()\n        plt.savefig(os.path.join(OUTPUT_DIR, f'shap_summary_class_{class_name}.png'), dpi=FIGURE_DPI)\n        plt.show()\n    print(f\"Saved class-specific plots for: {class_labels}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 SHAP Bar Plot (Mean Impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if USING_SHAP:\n    plt.figure(figsize=(12, 10))\n    shap.summary_plot(shap_values, X_shap_df, plot_type=\"bar\",\n                     class_names=class_labels, show=False, max_display=TOP_N_FEATURES)\n    plt.title('SHAP Mean Impact on Output', fontsize=16, fontweight='bold', pad=20)\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'shap_bar_plot.png'), dpi=FIGURE_DPI)\n    print(\"Saved: shap_bar_plot.png\")\n    plt.show()\n\n    print(\"\\nSHAP analysis complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Pipeline Summary\n",
    "\n",
    "Final summary of results and saved outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"PIPELINE COMPLETE - SUMMARY\")\nprint(\"=\"*70)\nprint(f\"\\nProcessed {len(mat_files)} ECG records\")\nprint(f\"Extracted {len(feature_cols)} features per record\")\nprint(f\"Preprocessing: {'Enabled' if APPLY_PREPROCESSING else 'Disabled'}\")\nprint(f\"Balancing (SMOTE): {'Applied' if APPLY_BALANCING else 'Skipped'}\")\nprint(f\"Model: Random Forest ({N_ESTIMATORS} trees)\")\nprint(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint(f\"\\nResults saved to: {OUTPUT_DIR}/\")\nprint(\"  - performance_metrics.csv\")\nprint(\"  - confusion_matrices.png\")\nprint(\"  - roc_curves.png\")\nprint(\"  - feature_importance.png\")\nprint(\"  - feature_importance.csv\")\nif USING_SHAP:\n    print(\"  - shap_summary_all_classes.png\")\n    print(\"  - shap_summary_class_*.png (per class)\")\n    print(\"  - shap_bar_plot.png\")\nprint(\"\\n\" + \"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}