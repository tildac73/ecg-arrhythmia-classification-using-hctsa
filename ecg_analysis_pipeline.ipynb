{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Arrhythmia Classification Pipeline\n",
    "\n",
    "**Complete pipeline for ECG analysis:**\n",
    "1. Feature extraction from ECG signals\n",
    "2. Preprocessing and filtering\n",
    "3. Class balancing (SMOTE)\n",
    "4. Random Forest classification\n",
    "5. Performance evaluation\n",
    "6. SHAP interpretability analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import scipy.signal as signal\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, accuracy_score,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pycatch22\n",
    "import shap\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters\n",
    "\n",
    "DATA_DIR describes the directory of the data we are using. To utilise this, please download the physionet ECG files using one of the following methods:\n",
    "\n",
    "1. Download the ZIP file (2.3 GB) from https://physionet.org/content/ecg-arrhythmia/1.0.0/ \n",
    "2. Download the files using your terminal: wget -r -N -c -np https://physionet.org/files/ecg-arrhythmia/1.0.0/\n",
    "3. Download the files using AWS command line tools: aws s3 sync --no-sign-request s3://physionet-open/ecg-arrhythmia/1.0.0/ DESTINATION\n",
    "\n",
    "If using the default config, place these files in a directory named \"./physionet_ecg_arrhythmia_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./physionet_ecg_arrhythmia_data\"\n",
    "DATA_DIR = \"./physionet_ecg_arrhythmia_data/WFDBRecords\"\n",
    "OUTPUT_DIR = \"./ecg_analysis_results\"\n",
    "\n",
    "# Preprocessing options\n",
    "APPLY_BALANCING = True\n",
    "\n",
    "# Signal processing parameters\n",
    "SAMPLING_RATE = 500\n",
    "BASELINE_CUTOFF = 0.5\n",
    "LOWPASS_CUTOFF = 40\n",
    "NOTCH_FREQ = 60\n",
    "WAVELET_TYPE = 'db6'\n",
    "WAVELET_LEVEL = 3\n",
    "\n",
    "# Machine learning parameters\n",
    "N_ESTIMATORS = 100\n",
    "TEST_SIZE = 0.3\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Visualization parameters\n",
    "SHAP_SAMPLE_SIZE = 300\n",
    "TOP_N_FEATURES = 20\n",
    "FIGURE_DPI = 150\n",
    "FONT_SIZE = 12\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ECG ANALYSIS PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nData directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"SMOTE balancing: {'Enabled' if APPLY_BALANCING else 'Disabled'}\")\n",
    "print(f\"Random Forest estimators: {N_ESTIMATORS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Functions\n",
    "\n",
    "Functions for ECG signal preprocessing and SNOMED-CT to AAMI class mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ecg_signal(ecg_signal, fs=SAMPLING_RATE):\n",
    "    \"\"\"Preprocess ECG signal with filtering and wavelet denoising.\"\"\"\n",
    "    try:\n",
    "        # Step 1: Baseline Wander Removal using High-pass Filter\n",
    "        # Baseline wander has a low frequency (typically below 0.5 Hz), so a high-pass\n",
    "        # filter effectively removes this drift while retaining the higher frequency\n",
    "        # components that represent the actual cardiac activity.\n",
    "        b, a = signal.butter(1, BASELINE_CUTOFF / (0.5 * fs), btype='highpass')\n",
    "        processed = signal.filtfilt(b, a, ecg_signal)\n",
    "\n",
    "        # Step 2: Filtering\n",
    "        # ECG signals contain important components primarily below 40 Hz, so by\n",
    "        # filtering out higher frequencies, you can reduce noise without affecting the\n",
    "        # diagnostic information in the ECG. This improves the signal quality and\n",
    "        # makes it easier to detect key features like QRS complexes and P-T waves.\n",
    "\n",
    "\n",
    "        # 2a: Low-pass filter to remove high-frequency noise\n",
    "        b, a = signal.butter(4, LOWPASS_CUTOFF / (0.5 * fs), btype='low')\n",
    "        processed = signal.filtfilt(b, a, processed)\n",
    "\n",
    "        # 2b: Notch filter to remove powerline interference at 50 or 60 Hz\n",
    "        b, a = signal.iirnotch(NOTCH_FREQ / (0.5 * fs), 30)\n",
    "        processed = signal.filtfilt(b, a, processed)\n",
    "\n",
    "        # Step 3: Denoising using Wavelet Transform\n",
    "        # Wavelet Denoising involves decomposing the ECG signal into different frequency\n",
    "        # components using wavelets, then selectively reducing noise by thresholding\n",
    "        # the wavelet coefficients, and finally reconstructing the signal.\n",
    "        # Wavelet denoising is particularly effective for signals like ECG because it can\n",
    "        # target noise at specific scales (frequencies) without affecting the signal’s\n",
    "        # main components. It’s adaptive and can handle non-stationary signals,\n",
    "        # which is a common characteristic of physiological data.\n",
    "        coeffs = pywt.wavedec(processed, WAVELET_TYPE, level=WAVELET_LEVEL)\n",
    "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
    "        uthresh = sigma * np.sqrt(2 * np.log(len(processed)))\n",
    "        denoised_coeffs = list(map(lambda x: pywt.threshold(x, uthresh, mode='soft'), coeffs))\n",
    "        processed = pywt.waverec(denoised_coeffs, WAVELET_TYPE)\n",
    "\n",
    "        # Normalise\n",
    "        return (processed - np.mean(processed)) / (np.std(processed) + 1e-10)\n",
    "    except:\n",
    "        return (ecg_signal - np.mean(ecg_signal)) / (np.std(ecg_signal) + 1e-10)\n",
    "\n",
    "print(\"Preprocessing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_snomed_to_aami(snomed_codes, db_path=BASE_DIR + \"/ecg-arrhythmia-1.0.0\"):\n",
    "    \"\"\"Map SNOMED-CT diagnostic codes to AAMI arrhythmia classes.\"\"\"\n",
    "    snomed_path = os.path.join(db_path, \"ConditionNames_SNOMED-CT.csv\")\n",
    "    snomed_dict = {}\n",
    "\n",
    "    if os.path.exists(snomed_path):\n",
    "        try:\n",
    "            snomed_df = pd.read_csv(snomed_path)\n",
    "            for _, row in snomed_df.iterrows():\n",
    "                snomed_dict[str(row['Snomed_CT'])] = row['Acronym Name']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    aami_map = {\n",
    "        'SR': 'N', 'SB': 'N', 'ST': 'N', 'SA': 'N', 'NSR': 'N',\n",
    "        'AFIB': 'S', 'AF': 'S', 'AT': 'S', 'AVNRT': 'S', 'AVRT': 'S',\n",
    "        'SVT': 'S', 'APB': 'S', 'ABI': 'S', 'SAAWR': 'S', 'JPT': 'S',\n",
    "        'JEB': 'S', '1AVB': 'S', '2AVB': 'S', '2AVB1': 'S', '2AVB2': 'S', 'AVB': 'S',\n",
    "        'VPB': 'V', 'VEB': 'V', 'VB': 'V', 'VET': 'V', 'VPE': 'V',\n",
    "        '3AVB': 'V', 'RBBB': 'V', 'LBBB': 'V', 'LFBBB': 'V', 'LBBBB': 'V',\n",
    "        'IVB': 'V', 'IDC': 'V',\n",
    "        'VFW': 'F',\n",
    "    }\n",
    "\n",
    "    aami_classes = []\n",
    "    for code in snomed_codes:\n",
    "        code = str(code).strip()\n",
    "        if code in snomed_dict:\n",
    "            acronym = snomed_dict[code]\n",
    "            aami_class = aami_map.get(acronym, 'Q')\n",
    "            aami_classes.append(aami_class)\n",
    "        else:\n",
    "            aami_classes.append('Q')\n",
    "\n",
    "    if 'V' in aami_classes:\n",
    "        return 'V'\n",
    "    if 'S' in aami_classes:\n",
    "        return 'S'\n",
    "    if 'F' in aami_classes:\n",
    "        return 'F'\n",
    "    if 'Q' in aami_classes:\n",
    "        return 'Q'\n",
    "    return 'N'\n",
    "\n",
    "print(\"SNOMED-CT mapping function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_ecg(file_path):\n",
    "    \"\"\"Extract catch22 time-series features from ECG signals.\"\"\"\n",
    "    features = []\n",
    "    try:\n",
    "        hea_path = file_path.replace('.mat', '.hea')\n",
    "        aami_class = 'Q'\n",
    "        dx_codes = []\n",
    "\n",
    "        if os.path.exists(hea_path):\n",
    "            with open(hea_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.startswith('#Dx:'):\n",
    "                        dx_str = line.strip().replace('#Dx:', '')\n",
    "                        dx_codes = [code.strip() for code in dx_str.split(',')]\n",
    "                        break\n",
    "\n",
    "        if dx_codes:\n",
    "            aami_class = map_snomed_to_aami(dx_codes)\n",
    "\n",
    "        mat_data = sio.loadmat(file_path)\n",
    "        signal_data = None\n",
    "\n",
    "        for key in ['val', 'data', 'signal']:\n",
    "            if key in mat_data:\n",
    "                signal_data = mat_data[key]\n",
    "                break\n",
    "\n",
    "        if signal_data is None:\n",
    "            for key, value in mat_data.items():\n",
    "                if isinstance(value, np.ndarray) and value.size > 0 and not key.startswith('__'):\n",
    "                    signal_data = value\n",
    "                    break\n",
    "\n",
    "        if signal_data is None:\n",
    "            return None\n",
    "\n",
    "        if signal_data.shape[0] > signal_data.shape[1]:\n",
    "            signal_data = signal_data.T\n",
    "\n",
    "        file_id = os.path.basename(file_path).replace('.mat', '')\n",
    "        n_leads = signal_data.shape[0]\n",
    "\n",
    "        for lead_idx in range(n_leads):\n",
    "            lead_signal = signal_data[lead_idx, :].astype(float)\n",
    "\n",
    "            if len(lead_signal) < 10 or np.std(lead_signal) < 1e-10:\n",
    "                continue\n",
    "\n",
    "            if np.count_nonzero(lead_signal) < 0.5 * len(lead_signal):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                lead_signal_processed = preprocess_ecg_signal(lead_signal)\n",
    "\n",
    "                catch22_result = pycatch22.catch22_all(lead_signal_processed)\n",
    "                feature_values = catch22_result['values']\n",
    "                feature_names = catch22_result['names']\n",
    "\n",
    "                for feat_name, feat_val in zip(feature_names, feature_values):\n",
    "                    if feat_val is None or (isinstance(feat_val, (int, float)) and (np.isinf(feat_val) or np.isnan(feat_val))):\n",
    "                        feat_val = np.nan\n",
    "\n",
    "                    features.append({\n",
    "                        'record_id': file_id,\n",
    "                        'lead': lead_idx,\n",
    "                        'feature_name': str(feat_name),\n",
    "                        'feature_value': float(feat_val) if not np.isnan(feat_val) else np.nan,\n",
    "                        'class': aami_class\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction from ECG Files\n",
    "\n",
    "Process all ECG files and extract catch22 time-series features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 1: FEATURE EXTRACTION\")\n",
    "\n",
    "# Find all .mat files with corresponding .hea files\n",
    "mat_files = []\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith('.mat') and not file.startswith('.'):\n",
    "            hea_file = file.replace('.mat', '.hea')\n",
    "            if os.path.exists(os.path.join(root, hea_file)):\n",
    "                mat_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(mat_files)} ECG files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all files\n",
    "all_features = []\n",
    "class_counts = Counter()\n",
    "\n",
    "for i, file_path in enumerate(mat_files):\n",
    "    if (i + 1) % 10 == 0 or (i + 1) == len(mat_files):\n",
    "        print(f\"Processing {i+1}/{len(mat_files)}: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    features = extract_features_from_ecg(file_path)\n",
    "    if features:\n",
    "        all_features.extend(features)\n",
    "        class_counts[features[0]['class']] += 1\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "print(f\"\\nExtracted {len(features_df)} feature values\")\n",
    "print(f\"Class distribution: {dict(class_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Preparation\n",
    "\n",
    "Transform features into wide format and prepare for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 2: FEATURE PREPARATION\")\n",
    "\n",
    "# Pivot to wide format\n",
    "features_wide = features_df.pivot_table(\n",
    "    index='record_id',\n",
    "    columns='feature_name',\n",
    "    values='feature_value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "class_mapping = features_df.groupby('record_id')['class'].first()\n",
    "features_wide['class'] = features_wide['record_id'].map(class_mapping)\n",
    "\n",
    "print(f\"Feature matrix shape: {features_wide.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "display(features_wide.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "feature_cols = [col for col in features_wide.columns if col not in ['record_id', 'class']]\n",
    "X = features_wide[feature_cols].values\n",
    "y = features_wide['class'].values\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Original class counts: {Counter(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Class Balancing with SMOTE\n",
    "\n",
    "Apply Synthetic Minority Over-sampling Technique to balance classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 3: CLASS BALANCING\")\n",
    "\n",
    "if APPLY_BALANCING:\n",
    "    try:\n",
    "        class_counts = Counter(y)\n",
    "        min_samples = min(class_counts.values())\n",
    "\n",
    "        if min_samples >= 2:\n",
    "            k_neighbors = min(1, min_samples - 1)\n",
    "\n",
    "            smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=k_neighbors)\n",
    "            X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "            print(f\"Applied SMOTE balancing\")\n",
    "            print(f\"Balanced class counts: {Counter(y_balanced)}\")\n",
    "        else:\n",
    "            print(\"Not enough samples for SMOTE, using original data\")\n",
    "            X_balanced, y_balanced = X, y\n",
    "    except Exception as e:\n",
    "        print(f\"SMOTE failed ({e}), using original data\")\n",
    "        X_balanced, y_balanced = X, y\n",
    "else:\n",
    "    print(\"Skipping balancing\")\n",
    "    X_balanced, y_balanced = X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 4: TRAIN RANDOM FOREST CLASSIFIER\")\n",
    "\n",
    "# Split data\n",
    "class_counts = Counter(y_balanced)\n",
    "min_class_count = min(class_counts.values())\n",
    "\n",
    "if min_class_count >= 2:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_balanced, y_balanced,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_balanced\n",
    "    )\n",
    "else:\n",
    "    print(\"Too few samples for stratified split, using random split\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_balanced, y_balanced,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest\n",
    "print(f\"\\nTraining Random Forest (n_estimators={N_ESTIMATORS})...\")\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics\n",
    "\n",
    "Calculate detailed performance metrics including sensitivity, specificity, PPV, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 5: EVALUATION METRICS\")\n",
    "\n",
    "class_labels = sorted(np.unique(y_balanced))\n",
    "cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate detailed metrics\n",
    "metrics_list = []\n",
    "for i, label in enumerate(class_labels):\n",
    "    tp = cm[i, i]\n",
    "    fn = np.sum(cm[i, :]) - tp\n",
    "    fp = np.sum(cm[:, i]) - tp\n",
    "    tn = np.sum(cm) - (tp + fn + fp)\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * (ppv * sensitivity) / (ppv + sensitivity) if (ppv + sensitivity) > 0 else 0\n",
    "\n",
    "    metrics_list.append({\n",
    "        'Class': label,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'PPV': ppv,\n",
    "        'F1_Score': f1\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "display(metrics_df.round(4))\n",
    "\n",
    "# Save results\n",
    "metrics_df.to_csv(os.path.join(OUTPUT_DIR, 'performance_metrics.csv'), index=False)\n",
    "print(f\"\\nSaved metrics to: {OUTPUT_DIR}/performance_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizations\n",
    "\n",
    "Generate confusion matrices, ROC curves, and feature importance plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 6: GENERATING VISUALIZATIONS\")\n",
    "\n",
    "plt.rcParams.update({'font.size': FONT_SIZE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels,\n",
    "            square=True, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted', fontweight='bold')\n",
    "axes[0].set_ylabel('True', fontweight='bold')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
    "\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Greens',\n",
    "            xticklabels=class_labels, yticklabels=class_labels,\n",
    "            square=True, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted', fontweight='bold')\n",
    "axes[1].set_ylabel('True', fontweight='bold')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrices.png'), dpi=FIGURE_DPI)\n",
    "print(\"Saved: confusion_matrices.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(class_labels) > 1:\n",
    "    lb = LabelBinarizer()\n",
    "    y_test_bin = lb.fit_transform(y_test)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "\n",
    "    for i, (color, label) in enumerate(zip(colors[:len(class_labels)], class_labels)):\n",
    "        if i < y_test_bin.shape[1] and i < y_pred_proba.shape[1]:\n",
    "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, color=color, lw=2,\n",
    "                     label=f'Class {label} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "    plt.title('ROC Curves', fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'roc_curves.png'), dpi=FIGURE_DPI)\n",
    "    print(\"Saved: roc_curves.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = rf_classifier.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_n = min(20, len(importance_df))\n",
    "top_features = importance_df.head(top_n)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'].values[::-1])\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'].values[::-1])\n",
    "plt.xlabel('Importance', fontweight='bold')\n",
    "plt.ylabel('Feature', fontweight='bold')\n",
    "plt.title(f'Top {top_n} Feature Importance', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'feature_importance.png'), dpi=FIGURE_DPI)\n",
    "print(\"Saved: feature_importance.png\")\n",
    "plt.show()\n",
    "\n",
    "importance_df.to_csv(os.path.join(OUTPUT_DIR, 'feature_importance.csv'), index=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "display(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SHAP Analysis (Interpretability)\n",
    "\n",
    "Use SHAP (SHapley Additive exPlanations) to understand feature contributions to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 7: SHAP ANALYSIS\")\n",
    "\n",
    "sample_size = min(SHAP_SAMPLE_SIZE, len(X_test_scaled))\n",
    "X_shap_sample = X_test_scaled[:sample_size]\n",
    "y_shap_sample = y_test[:sample_size]\n",
    "\n",
    "print(f\"\\nCalculating SHAP values for {sample_size} samples...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "explainer = shap.TreeExplainer(rf_classifier)\n",
    "shap_values = explainer.shap_values(X_shap_sample)\n",
    "\n",
    "X_shap_df = pd.DataFrame(X_shap_sample, columns=feature_cols)\n",
    "\n",
    "print(\"SHAP values calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 SHAP Summary Plot (All Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating SHAP summary plot...\")\n",
    "plt.figure(figsize=(14, 12))\n",
    "shap.summary_plot(shap_values, X_shap_df, class_names=class_labels,\n",
    "                    show=False, max_display=TOP_N_FEATURES)\n",
    "plt.title('SHAP Feature Importance - All Classes', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'shap_summary_all_classes.png'), dpi=FIGURE_DPI)\n",
    "print(\"Saved: shap_summary_all_classes.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 SHAP Class-Specific Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating class-specific SHAP plots...\")\n",
    "for i, class_name in enumerate(class_labels):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    shap.summary_plot(shap_values[i], X_shap_df, show=False, max_display=TOP_N_FEATURES)\n",
    "    plt.title(f'SHAP Feature Importance - Class {class_name}', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f'shap_summary_class_{class_name}.png'), dpi=FIGURE_DPI)\n",
    "    plt.show()\n",
    "print(f\"Saved class-specific plots for: {class_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 SHAP Bar Plot (Mean Impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_shap_df, plot_type=\"bar\",\n",
    "                    class_names=class_labels, show=False, max_display=TOP_N_FEATURES)\n",
    "plt.title('SHAP Mean Impact on Output', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'shap_bar_plot.png'), dpi=FIGURE_DPI)\n",
    "print(\"Saved: shap_bar_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSHAP analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Pipeline Summary\n",
    "\n",
    "Final summary of results and saved outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nProcessed {len(mat_files)} ECG records\")\n",
    "print(f\"Extracted {len(feature_cols)} features per record\")\n",
    "print(f\"Preprocessing: {'Enabled' if APPLY_PREPROCESSING else 'Disabled'}\")\n",
    "print(f\"Balancing (SMOTE): {'Applied' if APPLY_BALANCING else 'Skipped'}\")\n",
    "print(f\"Model: Random Forest ({N_ESTIMATORS} trees)\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"\\nResults saved to: {OUTPUT_DIR}/\")\n",
    "print(\"  - performance_metrics.csv\")\n",
    "print(\"  - confusion_matrices.png\")\n",
    "print(\"  - roc_curves.png\")\n",
    "print(\"  - feature_importance.png\")\n",
    "print(\"  - feature_importance.csv\")\n",
    "print(\"  - shap_summary_all_classes.png\")\n",
    "print(\"  - shap_summary_class_*.png (per class)\")\n",
    "print(\"  - shap_bar_plot.png\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
